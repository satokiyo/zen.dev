---
title: "Datastream for BigQueryã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†å‰²ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã™ã‚‹æ–¹æ³•ãƒ¡ãƒ¢"
emoji: "ğŸ‘€"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["Datastream", "BigQuery"]
published: true
---

# Datastream for BigQuery ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€£æºã™ã‚‹éš›ã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†å‰²ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æŒ‡å®šã§ããªã„ã€‚

Datastream ã®ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ç”»é¢ã‹ã‚‰ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®è¨­å®šã¯å‡ºæ¥ã¾ã›ã‚“ã§ã—ãŸã€‚

## è©¦ã—ãŸæ–¹æ³•

### 1. æœ€åˆã«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†å‰²ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¦ãŠã„ã¦ã€ãã“ã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã™ã‚‹

æœ€åˆã«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†å‰²ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åŒã˜åå‰ã§ç”¨æ„ã—ã¦ãŠã„ã¦

```bash
bq mk --table \
--schema ./schema.json \
--time_partitioning_field <field name> \
--require_partition_filter \
<project_id:dataset.table>
```

ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†å‰²ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆå¾Œã€ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’é–‹å§‹ã—ã¦ã¿ã‚‹ãŒã€ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹ã€‚

```text
Discarded 1265 unsupported events for BigQuery destination: <table>, with reason code: BIGQUERY_UNSUPPORTED_PRIMARY_KEY_CHANGE, details: Failed to write to BigQuery due to an unsupported primary key change: adding primary keys to existing tables is not supported..
```

ä»¥ä¸‹ã®è¨˜äº‹ã‚’å‚è€ƒã«ã€
https://medium.com/google-cloud/configure-streams-in-datastream-with-predefined-tables-in-bigquery-for-postgresql-as-source-528340f7989b

ã“ã® SQL ã‚’å®Ÿè¡Œã—ã¦ã¿ãŸãŒã‚¨ãƒ©ãƒ¼ã€ã€

```bash
bq query --use_legacy_sql=false '
CREATE OR REPLACE TABLE  `project_id.dataset.table`
  (
  id INTEGER,
  ...
  datastream_metadata STRUCT<uuid STRING, source_timestamp INT64>,
    PRIMARY KEY (id) NOT ENFORCED)
PARTITION BY <partitioning field name>
CLUSTER BY <clustering field name>
OPTIONS(max_staleness = INTERVAL 15 MINUTE);
'
#  BigQuery error in query operation: Error processing job '': Incompatible table partitioning specification. Expected partitioning specification  clustering(id), but input partitioning specification is interval(type:day,field:<partitioning field name>) clustering(<clustering field name>)
```

â‡’ id åˆ—ã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ—ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ—ã®æŒ‡å®šãŒé–“é•ã£ã¦ã„ã‚‹ã½ã„ãŒã€ã‚ˆãåˆ†ã‹ã‚‰ãªã„ã€‚ã€‚

### 2. ã‚¹ãƒˆãƒªãƒ¼ãƒ ä½œæˆå¾Œã« update ã—ã¦ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŒ–ã‚’è©¦ã¿ã‚‹

æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—

```bash
bq show \
--schema \
--format=prettyjson \
project_id:dataset.table > schema.json
```

æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒã‚’æŒ‡å®šã—ã¦ update ã—ã¦ã¿ã‚‹

```bash
bq update --table --schema ./schema.json --time_partitioning_field <field name> --require_partition_filter <project_id:dataset.table>
#  BigQuery error in update operation: Cannot convert non partitioned table to partitioned table.
```

â‡’ ä¸Šã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã€‚å¾Œã‹ã‚‰ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†å‰²ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¤‰æ›´ã™ã‚‹ã“ã¨ã¯ã§ããªã„!

### 3. ã‚¹ãƒˆãƒªãƒ¼ãƒ ä½œæˆå¾Œã«ã€æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¸€æ—¦ GCS ã«ã‚³ãƒ”ãƒ¼ã—ã¦ã€åˆ¥åã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†å‰²ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã€ãã®å¾Œãƒ‡ãƒ¼ã‚¿ã‚’æˆ»ã—ã¦ã‹ã‚‰ãƒªãƒãƒ¼ãƒ ã™ã‚‹

ã“ã®è¨˜äº‹ã‚’å‚è€ƒã«ã™ã‚‹ã¨ã€
https://medium.com/google-cloud/how-to-modify-bigquery-table-definition-at-no-costs-2c59a1073cbb

BigQuery å†…ã§ã® SQL ã ã‘ã§ã‚‚å‡ºæ¥ã‚‹ãŒã€ãƒ•ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ãŒç™ºç”Ÿã—ã¦ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ã€‚ãã“ã§ GCS ã‚’çµŒç”±ã™ã‚‹ã“ã¨ã§ãƒ•ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ã‚’é¿ã‘ã¦ãƒãƒ¼ã‚³ã‚¹ãƒˆã§ã„ã‘ã‚‹ï¼

è©¦ã—ã¦ã¿ã‚‹ã€‚

```bash
export MY_BUCKET=<bucket name>
export PROJECT_NAME=<project name>
export DATASET=<dataset name>
export TABLE=<table name>

---
# Create a new Google Cloud Storage (GCS) bucket
gsutil mb -l asia-northeast1 -p ${PROJECT_NAME}  gs://${MY_BUCKET}

---
# Dump the content of the table to the bucket
bq extract \
--destination_format NEWLINE_DELIMITED_JSON \
--print_header=true \
${PROJECT_NAME}:${DATASET}.${TABLE}  gs://${MY_BUCKET}/${TABLE}*
# It is important to provide wildcard * at the end of the filename, because it is not possible to extract from BQ to GCS table of size bigger than 1GB. If we specify *, bq extract will split the exported data to multiple files.

---
# Export the schema of the original table. It will be used in the next step.
bq show \
--format=prettyjson \
${PROJECT_NAME}:${DATASET}.${TABLE} \
| jq '.schema.fields' > schema_${TABLE}.json

---
# Load the data to the new BQ table which is partitioned hourly and it's schema matches the original table.
bq load \
--source_format=NEWLINE_DELIMITED_JSON \
--time_partitioning_type=DAY \
--time_partitioning_field=<partitioning field name> \
--clustering_fields <clustering field name> \
${PROJECT_NAME}:${DATASET}.${TABLE}_partitioned \
gs://${MY_BUCKET}/${TABLE}* ./schema_${TABLE}.json
# Note, there is a limitation to the bq load, that a single job can load at most 15 TB. If our data is bigger, we need to split the payload to multiple load jobs.

---
# Switch tables. Remove the original one
bq rm ${PROJECT_NAME}:${DATASET}.${TABLE} && \
bq query --use_legacy_sql=false "ALTER TABLE ${DATASET}.${TABLE}_partitioned RENAME TO ${TABLE};"

# Clean up. Delete the bucket to ensure that you do not pay for storage costs
gsutil -m rm gs://${MY_BUCKET}
```

â‡’ ä¸Šè¨˜å®Ÿè¡Œå¾Œã€ãƒ­ã‚®ãƒ³ã‚°ã‚’ç¢ºèªã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ãª Warning ãŒå‡ºã¦ä¸Šæ‰‹ãã„ã£ã¦ã„ãªã„ã€‚ã€‚

```text
BIGQUERY_UNSUPPORTED_PRIMARY_KEY_CHANGE, details: Failed to write to BigQuery due to an unsupported primary key change: adding primary keys to existing tables is not supported
```

ã‚„ã¯ã‚Šå¾Œã‹ã‚‰ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†å‰²ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã—ã¦ã‚‚ã€ä¸Šæ‰‹ãã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ããªã„ã‚ˆã†ã ã€‚ã€‚

(è¿½è¨˜)
ã“ã®å…¬å¼ãƒšãƒ¼ã‚¸ã«ã‚‚ä¸€æ—¦ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’åœæ­¢ã—ã¦ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã„ã†ã€ä¸Šã§è©¦ã—ãŸæ–¹æ³•ãŒæ›¸ã„ã¦ã‚ã‚‹ãŒã€
https://cloud.google.com/datastream/docs/best-practices-stream-data#partition-replica-datasets
Datastream for BigQuery(preview)ã ã¨ã€ä¸Šæ‰‹ãã„ã‹ãªã‹ã£ãŸã€‚ã€‚
å°‘ãªãã¨ã‚‚ç¾çŠ¶ã¯ã€Datastream -> GCS -> pubsub -> Dataflow -> BigQuery ã® Google æä¾›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ã†å ´åˆã«é™ã‚‰ã‚Œã‚‹ã¿ãŸã„ï¼Ÿ

## è§£æ±ºã—ãŸæ–¹æ³•

ä¸‹è¨˜ã® ã‚¤ã‚·ãƒ¥ãƒ¼ãƒˆãƒ©ãƒƒã‚«ãƒ¼ ã‚’è¦‹ã‚‹ã¨ã€ä¸Šè¨˜ 3.ã®æ–¹æ³•ã¯ã€ã€ŒDatastream ã® CDC ã§ã¯ã€å¯¾å¿œã§ããªã„ã‚‰ã—ã„ã€‚

> however this approach won't work for existing Datastream sourced tables **since there wouldn't be a \_CHANGE_SEQUENCE_NUMBER field which is required to correctly apply UPSERT operations in the correct order**. So the only option would be to pre-create the table with partitioning/clustering/primary keys before starting the Datastream stream like the below DDL SQL sample query.ã€

ã‚¤ã‚·ãƒ¥ãƒ¼ãƒˆãƒ©ãƒƒã‚«ãƒ¼
https://issuetracker.google.com/issues/250938168

ä¸Šè¨˜ã‚¤ã‚·ãƒ¥ãƒ¼ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’è¦‹ã‚‹ã¨ã€ä»¥ä¸‹ã® DDL ã‚’ã—ã¦ã‹ã‚‰ã‚¹ãƒˆãƒªãƒ¼ãƒ ã™ã‚Œã°ã„ã„ã¨æ›¸ã„ã¦ã‚ã‚‹ã€‚ã‚ˆãè¦‹ã‚‹ã¨ã€**Primarykey ã¨ã€Clustering ã«åŒã˜ã‚«ãƒ©ãƒ ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒãƒã‚¹ãƒˆ**ã®ã‚ˆã†ã ã€‚

> CLUSTER BY
> Primary_key_field #This must be an exact match of the specified primary key fields

ã“ã‚Œã‚’è©¦ã—ã¦ã¿ã‚‹

```bash
bq query --use_legacy_sql=false "CREATE OR REPLACE TABLE ${DATASET}.${TABLE} (
   id INTEGER PRIMARY KEY NOT ENFORCED,
   ...
   datastream_metadata STRUCT<uuid STRING, source_timestamp INT64> )
PARTITION BY
 <partitioning field name>
CLUSTER BY
  id
OPTIONS(max_staleness = INTERVAL 15 MINUTE);
"
```

â‡’ ã“ã‚Œã§ä¸Šæ‰‹ãã„ã£ãŸ!
